{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie niezbędnych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyMorfologik import Morfologik\n",
    "from pyMorfologik.parsing import ListParser\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.layers import Conv1D, Flatten, Dropout, Dense, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicja klasy procesującej dokumenty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedDataFrame():\n",
    "    __slots__ = ['stopwords', 'stemmed_dict']\n",
    "    \n",
    "    def __init__(self, stopwords):\n",
    "        self.stopwords = stopwords\n",
    "        self.stemmed_dict = None\n",
    "        \n",
    "    \n",
    "    def prepare_text(self, doc):\n",
    "        doc = re.sub(r'[^\\w\\s]','',doc)\n",
    "        doc = doc.lower()\n",
    "        doc = nltk.word_tokenize(doc)\n",
    "        doc = [word for word in doc if word not in self.stopwords]\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def apply_stemming(self, df):\n",
    "        \n",
    "        parser = ListParser()\n",
    "        stemmer = Morfologik()\n",
    "        \n",
    "        all_words = list()\n",
    "\n",
    "        for item in list(df):\n",
    "            all_words.extend(item)\n",
    "    \n",
    "        unique_words = list(set(all_words))\n",
    "        \n",
    "        self.stemmed_dict = dict()\n",
    "        unique_words_stemmer = stemmer.stem(unique_words, parser)\n",
    "\n",
    "        for item in unique_words_stemmer:\n",
    "            original = item[0]\n",
    "            try:\n",
    "                stemmed = list(item[1])[0]\n",
    "                self.stemmed_dict[original] = stemmed\n",
    "            except IndexError:\n",
    "                self.stemmed_dict[original] = stemmed\n",
    "        \n",
    "        \n",
    "        missings = {item:item for item in unique_words if item not in self.stemmed_dict.keys()}\n",
    "        self.stemmed_dict.update(missings)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def remove_empty(df, col):\n",
    "        return df.loc[df[col].apply(lambda x: len(x)) > 0, :]\n",
    "    \n",
    "       \n",
    "    def transform(self, X, col, **kwargs):\n",
    "        try:\n",
    "            X_local = X.copy()\n",
    "            X_local[col] = X_local[col].apply(self.prepare_text)\n",
    "            X_local = self.remove_empty(X_local, col)\n",
    "        except KeyError:\n",
    "            raise KeyError(\"{} not present in dataframe\".format(col))\n",
    "            \n",
    "        if not self.stemmed_dict:\n",
    "            self.apply_stemming(X_local[col])\n",
    "            \n",
    "        X_local[col] = X_local[col].apply(lambda doc: list(map(self.stemmed_dict.get, doc)))\n",
    "        X_local = X_local.reset_index(drop=True)\n",
    "        \n",
    "        return X_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zdefiniowanie stałych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/learning_set.csv'\n",
    "STOPWORDS_PATH = 'data/polish_stopwords.csv'\n",
    "W2V_PATH = 'data/nkjp+wiki-forms-all-100-cbow-hs.txt'\n",
    "\n",
    "RANDOM_STATE = 23032019\n",
    "\n",
    "C_V = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "SCORING = 'accuracy'\n",
    "N_JOBS = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sentiment').count()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(pd.read_csv(STOPWORDS_PATH, engine='python', header=None).iloc[:, 0])\n",
    "\n",
    "\n",
    "df = TokenizedDataFrame(stopwords).transform(df, 'token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podział na zbiór treningowy/testowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### y_train/y_test używamy do sklearn API, dummy do Kerasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.token, \n",
    "                                                    df.sentiment, \n",
    "                                                    test_size=0.15, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=RANDOM_STATE)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "dummy_y_train = np_utils.to_categorical(encoded_y_train)\n",
    "dummy_y_test = np_utils.to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer='word', tokenizer=lambda x: x,\n",
    "                              preprocessor=lambda x: x, ngram_range=(1,2),\n",
    "                              sublinear_tf=True)),\n",
    "    ('lr', LogisticRegression(penalty='l2', random_state=RANDOM_STATE,\n",
    "                              n_jobs=-1, multi_class='multinomial', solver='lbfgs')\n",
    "                             )\n",
    "])\n",
    "\n",
    "model_lr.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ile wynosi null accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:.1f}%\".format(100*y_train.value_counts().max()/len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jakie accuracy na 10-krotnej CV osiąga regresja logistyczna?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(model_lr, \n",
    "               x_train,\n",
    "               y_train,\n",
    "               cv=C_V,\n",
    "               scoring=SCORING,\n",
    "               n_jobs=N_JOBS, \n",
    "               return_train_score=True)\\\n",
    ".get('test_score')\\\n",
    ".mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy na zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, model_lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykres krzywej uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.estimators.plot_learning_curve(model_lr,\n",
    "                                     x_train,\n",
    "                                     y_train,\n",
    "                                     cv=C_V,\n",
    "                                    random_state=RANDOM_STATE,\n",
    "                                    n_jobs=N_JOBS,\n",
    "                                    scoring=SCORING)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykres słów najbardziej stymulujących skrajny sentyment według regresji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = model_lr.named_steps.get('lr').coef_\n",
    "index = coefficients.argsort()\n",
    "\n",
    "feature_names = np.array(model_lr.named_steps.get('tfidf').get_feature_names())\n",
    "feature_names_comb = list(feature_names[index][0][:30]) + list(feature_names[index][0][-31::1])\n",
    "\n",
    "index_comb = list(coefficients[0][index[0][:30]]) + list(coefficients[0][index[0][-31::1]])\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "barlist = plt.bar(list(i for i in range(61)), index_comb)\n",
    "plt.xticks(list(i for i in range(61)),feature_names_comb,rotation=75,size=15)\n",
    "plt.ylabel('Coefficient magnitude',size=20)\n",
    "plt.xlabel('Features',size=20)\n",
    "\n",
    "# color the first smallest 30 bars red\n",
    "for i in range(30, 61):\n",
    "    barlist[i].set_color('red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zdefiniowanie funkcji wczytującej embeddingi, klasy procesującej dane do sieci neuronowej i samej sieci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(): # function to load word embedding only once \n",
    "    global w2vModel\n",
    "    w2vModel = load_w2v_embeddings(W2V_PATH)\n",
    "\n",
    "        \n",
    "def get_embeddings():\n",
    "    global w2vModel\n",
    "    try:\n",
    "        return w2vModel\n",
    "    except NameError:\n",
    "        init()\n",
    "        return w2vModel\n",
    "\n",
    "\n",
    "def load_w2v_embeddings(path):\n",
    "    w2vModel = gensim.models.KeyedVectors.load_word2vec_format(path, \n",
    "                                                               binary=False)\n",
    "    return w2vModel \n",
    "\n",
    "\n",
    "def plot_training(history): # plot training history\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak wczytać dane do challenge'u?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scoring_set = pd.read_csv(\"./data/challenge_set_warsztat.csv\", sep = ';')\n",
    "scoring_set = TokenizedDataFrame(stopwords).transform(scoring_set, 'token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak stworzyć i zapisać predykcję?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla sklearn API\n",
    "pred = model_lr.predict(scoring_set.token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla Keras\n",
    "pred = model.predict(scoring_set.token).argmax(axis=1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pred) == 5022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wektor zapisany przez np.save należy przesłać do nas na e-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('nickname', pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
